{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Nets in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Jeremy Locatelli, Erik Ring-Walters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aimed to create a recurrent neural net in TensorFlow based on Andrej Karpathy's code in plain Python. This net reads a set of characters (Alice's Adventures in Wonderland, for example) and generates text that aims to look closer to the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test our neural net on a smaller dataset, but one with a much different style, every line from the hit movie starring Jerry Seinfeld, The Bee Movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.reset_default_graph()\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 52729 characters, 69 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open('./beemovie.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "seq_length = 25\n",
    "\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data)-seq_length-1, 1):\n",
    "        X.append([char_to_ix[ch] for ch in data[i:i+seq_length]])\n",
    "        y.append([char_to_ix[ch] for ch in data[i+1:i+seq_length+1]])\n",
    "\n",
    "# reshape the data\n",
    "# in X_modified, each row is an encoded sequence of characters\n",
    "X_modified = np.reshape(X, (len(X), seq_length))\n",
    "y_modified = np.reshape(y, (len(y), seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @params: X: the input array to fetch a batch from\n",
    "# @params: y: the output array to fetch a corosponding batch from\n",
    "# @params: batch_size: The size of the batch you want to pull\n",
    "def fetchBatch(X,y,batch_size):\n",
    "    idx = np.random.randint((len(X)-batch_size)-1)\n",
    "    X_batch = X[idx:idx+batch_size,:]\n",
    "    y_batch = y[idx:idx+batch_size,:]\n",
    "    yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls a single random row from X_modifed\n",
    "def single_batch(X):\n",
    "    rnd = np.random.randint(len(X)-1)\n",
    "    return X[rnd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "n_inputs = vocab_size\n",
    "n_outputs = vocab_size\n",
    "n_steps = 25\n",
    "n_neurons = 100\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First problem we faced\n",
    "One of the first big design decisions we made was whether to one hot encode each batch in tensorflow, or one hot encode the entire input text, then create batches with that. We decided to one hot encode each batch because we had trouble creating a fetch batch function on the large one hot encoded 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, n_steps])\n",
    "one_hot_X = tf.one_hot(X, vocab_size)\n",
    "y = tf.placeholder(tf.int32, [None, n_steps])\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(\n",
    "        num_units=n_neurons, activation=tf.nn.relu),output_size=n_outputs)\n",
    "\n",
    "cpu_gru_cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.GRUBlockCellV2(\n",
    "        num_units=n_neurons),output_size=n_outputs)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, one_hot_X, dtype=tf.float32) \n",
    "learning_rate = 0.001\n",
    "y_probs = tf.nn.softmax(logits=outputs)\n",
    "xenthropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=y)\n",
    "loss = tf.reduce_mean(xenthropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling data\n",
    "#### Thanks Dr. Bruns\n",
    "This was the biggest breakthrough in terms of understanding, creating a \"floating window\" that tracked the updated sequence we feed to the RNN. Once we got this working it led to the discovery of several other issues. \n",
    "One was for some reason all of our outputs were converging towards a single character, e.g. i would call sample with length 100 and it would output 100 commas (or newline characters which does wonders for a console program..)\n",
    "I tried to solve this bug for hours, looking at the probability distributions for sequential predictions. Then we discovered in our fetch batch function, we accidentaly assigned both X and y_batch to the same values. Once we fixed that one character error, our output started to improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(seed_pred, n):\n",
    "    start_idx = 0\n",
    "    # list of indexes we predict\n",
    "    output = []\n",
    "    # Debug for seeing the prob distributions\n",
    "    probs_list = []\n",
    "    \n",
    "    for i in range(n):\n",
    "      # This creates the moving window that starts with our input sequence, and the new input is appended at each loop\n",
    "      window = seed_pred[start_idx:start_idx + n_steps]\n",
    "      # We then rotate it to batch_size = 1 and 25 cols for entry into the X tensor\n",
    "      window = window.reshape((1, n_steps))\n",
    "      # We evaluate the softmax of the outputs by inserting our array window into the network and getting the logits\n",
    "      char_prob = y_probs.eval(feed_dict={X:window})\n",
    "      probs_list.append(char_prob)\n",
    "      # We only care about the last character softmaxed outputs, as this is the probability of a choice of one of len(vocab_size) characters\n",
    "      last_char = char_prob[0][n_steps-1]\n",
    "      # We get an index value by pulling a random value /according to the probabilty distribution of the last_chars softmax      \n",
    "      ind = np.random.choice(range(vocab_size), p=last_char.ravel())\n",
    "      # tack that onto our big list\n",
    "      output.append(ind)\n",
    "      # reshape ind so we can tack it to the bottom of seed_pred\n",
    "      ind = ind.reshape((1,))\n",
    "      # append it to seed_pred\n",
    "      seed_pred = np.append(seed_pred, ind, axis=0)\n",
    "      # move the window up one and continue\n",
    "      start_idx +=1\n",
    "    return output, probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Basic Cell RNN===\n",
      "Current epoch: 0 \tLoss: \t4.194098472595215\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      " fqd\n",
      "JIO!'b3L6e9Rg61JbWx0?zod!fQqG'\"6uLpF41,fwSfujGKf:MlS:, .Fd1Q\n",
      "HcP'2q067qgSrk:Egjjr-aDDHwG-fMD\"B!P \n",
      "===============\n",
      "Current epoch: 2500 \tLoss: \t2.0627481937408447\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      "  wat ia lesem thet wos? Whare ghe king.\n",
      "\n",
      " Nod youphntew shalk ar eass, yould taking in got on thing  \n",
      "===============\n",
      "Current epoch: 5000 \tLoss: \t2.2492544651031494\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      " yw.\n",
      "\n",
      " Yelaalild gat fiscarnound thecking coplong,\n",
      "\n",
      " wish everom fud in yrione! And ores, legingcis t \n",
      "===============\n",
      "Current epoch: 7500 \tLoss: \t1.8409218788146973\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      " y wis tian to cours!\n",
      "\n",
      " I'm right up you met wn this!\n",
      "\n",
      " Bresine adee all itrlabe an? Ana sie to say t \n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 50\n",
    "basic_losses = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"===Basic Cell RNN===\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in fetchBatch(X_modified,y_modified,batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            if epoch % 2500 == 0:\n",
    "                cur_loss = loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                basic_losses.append(cur_loss)\n",
    "                print(f\"Current epoch: {epoch} \\tLoss: \\t{cur_loss}\\n\")\n",
    "                solo_batch = single_batch(X_modified)\n",
    "                smpl, _ = sample(solo_batch,100)\n",
    "                txt = ''.join(ix_to_char[ix] for ix in smpl)\n",
    "                print(\"OUTPUT\")\n",
    "                print('===============\\n %s \\n===============' % (txt, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU RNN\n",
    "Here we build a RNN with the exact same architecture as the last one, but we use GRU Cells instead of basic RNN cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "X = tf.placeholder(tf.int32, [None, n_steps])\n",
    "one_hot_X = tf.one_hot(X, vocab_size)\n",
    "y = tf.placeholder(tf.int32, [None, n_steps])\n",
    "\n",
    "cpu_gru_cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.GRUBlockCellV2(\n",
    "        num_units=n_neurons),output_size=n_outputs)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cpu_gru_cell, one_hot_X, dtype=tf.float32) \n",
    "learning_rate = 0.001\n",
    "y_probs = tf.nn.softmax(logits=outputs)\n",
    "xenthropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=y)\n",
    "loss = tf.reduce_mean(xenthropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===GRU RNN===\n",
      "Current epoch: 0 \tLoss: \t4.198111057281494\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      " 7QHPuyNHE3\n",
      ":M5M8Lou,bgmRNfxO N:PE1\n",
      "VtddQnh3pf37skF.ISgt.M-9lhwTqL47ryjRzftf\n",
      "p,v\" a-9WqVzKsA',GUWIyg5 \n",
      "===============\n",
      "Current epoch: 2500 \tLoss: \t1.9203094244003296\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      " t.\n",
      ". - pell.\n",
      "\n",
      " - Nol.\n",
      "\n",
      " Barem. Io Orore seandysne I donyte cKut, steces a lid.\n",
      "\n",
      " You dor yeallthe bc \n",
      "===============\n",
      "Current epoch: 5000 \tLoss: \t1.3278095722198486\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      " s. You'se got if recphyther this is!\n",
      "\n",
      " He.\n",
      "\n",
      " Your fore it keol packnom theter asling her thm woren.  \n",
      "===============\n",
      "Current epoch: 7500 \tLoss: \t1.8070523738861084\n",
      "\n",
      "OUTPUT\n",
      "===============\n",
      " k lifter!\n",
      "\n",
      " So.\n",
      "\n",
      " Bet, I'd a joithit of have is ceaple a bel you out I gock it we ron't it, the Bair \n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 50\n",
    "lstm_losses = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"===GRU RNN===\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in fetchBatch(X_modified,y_modified,batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            if epoch % 2500 == 0:\n",
    "                cur_loss = loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                lstm_losses.append(cur_loss)\n",
    "                print(f\"Current epoch: {epoch} \\tLoss: \\t{cur_loss}\\n\")\n",
    "                solo_batch = single_batch(X_modified)\n",
    "                smpl, _ = sample(solo_batch,100)\n",
    "                txt = ''.join(ix_to_char[ix] for ix in smpl)\n",
    "                print(\"OUTPUT\")\n",
    "                print('===============\\n %s \\n===============' % (txt, ))\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build a LSTM cell rather than a GRU cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "X = tf.placeholder(tf.int32, [None, n_steps])\n",
    "one_hot_X = tf.one_hot(X, vocab_size)\n",
    "y = tf.placeholder(tf.int32, [None, n_steps])\n",
    "\n",
    "cpu_lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.GLSTMCell(\n",
    "        num_units=n_neurons),output_size=n_outputs)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cpu_lstm_cell, one_hot_X, dtype=tf.float32) \n",
    "learning_rate = 0.001\n",
    "y_probs = tf.nn.softmax(logits=outputs)\n",
    "xenthropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=y)\n",
    "loss = tf.reduce_mean(xenthropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 50\n",
    "lstm_losses = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"===LSTM RNN===\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in fetchBatch(X_modified,y_modified,batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            if epoch % 2500 == 0:\n",
    "                cur_loss = loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                lstm_losses.append(cur_loss)\n",
    "                print(f\"Current epoch: {epoch} \\tLoss: \\t{cur_loss}\\n\")\n",
    "                solo_batch = single_batch(X_modified)\n",
    "                smpl, _ = sample(solo_batch,100)\n",
    "                txt = ''.join(ix_to_char[ix] for ix in smpl)\n",
    "                print(\"OUTPUT\")\n",
    "                print('===============\\n %s \\n===============' % (txt, ))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 50\n",
    "gru_losses = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"===LSTM RNN===\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in fetchBatch(X_modified,y_modified,batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            if epoch % 1000 == 0:\n",
    "                cur_loss = loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                gru_losses.append(cur_loss)\n",
    "                print(f\"Current epoch: {epoch} \\tLoss: \\t{cur_loss}\\n\")\n",
    "                solo_batch = single_batch(X_modified)\n",
    "                smpl, _ = sample(solo_batch,100)\n",
    "                txt = ''.join(ix_to_char[ix] for ix in smpl)\n",
    "                print(\"OUTPUT\")\n",
    "                print('===============\\n %s \\n===============' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.arange(len(basic_losses))\n",
    "plt.plot(basic_losses, y, label='basic rnn loss')\n",
    "plt.plot(gru_losses, y, label='gru rnn loss')\n",
    "plt.plot(lstm_losses, y, label='lstm rnn loss')\n",
    "plt.legend();\n",
    "plt.title(\"RNN Loss\")\n",
    "plt.xlabel(\"output iteration\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we explored recurrent neural nets in TensorFlow. Using Karpathy's plain Python code as a baseline, we were able to create functioning neural nets that used \"The Bee Movie's\" script as input and output pieces of text that looked somewhat like movie script lines. The most difficult challenges we faced included understanding the geometry and proper dimensions of our data structures to properly work with our data, as well as ensuring our TensorFlow code was implementing proper variables when needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
